
#
# cENM Credemtialmanager Controller POD
#
# Ericsson - District11 - 2020
#

#################################
#                               #
#  JOB EXECUTION                #
#                               #
#################################

import sys
import time
import common
import constants
import k8sApiInterface
import restAPIutil
import restAPIsecretUtil
import restAPI
import restAPIms8ms9
#import random



###########################################
###########################################
#
#   THREAD TRANSITION ENABLING TO ENABLED
#
###########################################
###########################################



###########################################
# credmControllerTransitionFromEnablingToEnabled
#
# descr: perform all steps to let credm cnt go to enabled state:
#  - restart of sps deployment
#  - wait for the new "ready" pods that replace the previous ones
#  - t.b.d. : deletion of cert. for cli java application
#  - execution of an "internal" cron job
#  - set the credmEnableState field to "enabled" for eric-enm-credm-controller-state secret
#
# in: none
# out:  boolean
###########################################
def credmControllerTransitionFromEnablingToEnabled():

    common.log(constants.LOG_LEVEL_DEBUG, "credmControllerTransitionFromEnablingToEnabled: STARTING ...")
    result = True

    # check if controller state in valid state to do the transition from enabling state to enabled
    credmStateValue, cronStateValue, enableStateValue = restAPIms8ms9.getMonitoringState(allFlag=True)
    if not enableStateValue == constants.ENABLE_STATE['ENABLING']:
        common.log(constants.LOG_LEVEL_DEBUG, "credmControllerTransitionFromEnablingToEnabled: credm state not enabling. Nothing to do")
        return result

    # restart of sps deployment and wait for the new "ready" pods that replace the previous ones
    if result == True:
        result = __restartSpsDeploymentAndWaitForNewReadyPods()
        if result == False:
            common.log(constants.LOG_LEVEL_WARNING, "credmControllerTransitionFromEnablingToEnabled: __restartSpsDeploymentAndWaitForNewReadyPods failed")

    # deletion of certificates for cli java application:
    # after restart, it is possible that sps service has certificates/trusts signed by new ca certs;
    # therefore also certificates/trusts for cli java app. have to be generated by new ca certs.
    # Perhaps in any cases it is redundant ...
    if result == True:
        result = __emptyTlsSecretsForCli()
        if result == False:
            common.log(constants.LOG_LEVEL_WARNING, "credmControllerTransitionFromEnablingToEnabled: __emptyTlsSecretsForCli failed")

    # running an "internal" cronjob
    if result == True:
        result = __internalCronJob()
        if result == False:
            common.log(constants.LOG_LEVEL_WARNING, "credmControllerTransitionFromEnablingToEnabled: __internalCronJob failed")

    # set the value to "enabled" for the "credmEnableState" field of the "eric-enm-credm-controller-state" secret
    # NOTE: the field is set also if any previous operation failed
    resultSetCntState = restAPIms8ms9.setControllerState(credmState=constants.CREDMENABLE_STATE['ENABLED'])
    if resultSetCntState == False:
        common.log(constants.LOG_LEVEL_WARNING, "credmControllerTransitionFromEnablingToEnabled: __setControllerState failed")
        result = False

    if result == True:
        common.log(constants.LOG_LEVEL_DEBUG, "credmControllerTransitionFromEnablingToEnabled: ... END with SUCCESS")
    else:
        common.log(constants.LOG_LEVEL_WARNING, "credmControllerTransitionFromEnablingToEnabled: ... END with FAILURE")

    return result


###########################################
# __restartSpsDeploymentAndWaitForNewReadyPods
#
# descr: perform following steps:
#  - restart of sps deployment
#  - wait for the new "ready" pods that replace the previous ones
#
# in: none
# out:  boolean
###########################################
def __restartSpsDeploymentAndWaitForNewReadyPods():

    common.log(constants.LOG_LEVEL_DEBUG, "__restartSpsDeploymentAndWaitForNewReadyPods: STARTING ...")
    result = True

    # get sps old pods
    retList = restAPIutil.loadSpsPods()

    # min. one sps pod must exist
    if len(retList.items) == 0:
        result = False
        common.log(constants.LOG_LEVEL_WARNING, "__restartSpsDeploymentAndWaitForNewReadyPods: sps pods NOT found")

    # store the names of sps old pods
    spsPodsOldNames = []
    if result == True:
        for item in retList.items:
            spsPodsOldNames.append(item.metadata.name)
        common.log(constants.LOG_LEVEL_DEBUG, "__restartSpsDeploymentAndWaitForNewReadyPods; sps pods old names: %s" % (spsPodsOldNames))

    # restart sps deployment
    if result == True:
        result = k8sApiInterface.restartService(constants.SPS_APP_LABEL, constants.NAMESPACE)
        if result == False:
            common.log(constants.LOG_LEVEL_WARNING, "__restartSpsDeploymentAndWaitForNewReadyPods: unable to restart the service %s" % (constants.SPS_APP_LABEL))

            # wait for the sps new "ready" pods
    # 1st time step : to have all new pods (different names)
    if result == True:
        spsPodsNewNames = []
        while True:
            # wait for 45 sec for sps pods start-up
            common.log(constants.LOG_LEVEL_DEBUG, "__restartSpsDeploymentAndWaitForNewReadyPods: (phase 1) sleep %s sec before ckecking if all pods are new" % (constants.SPS_WAIT_STARTUP))
            time.sleep(constants.SPS_WAIT_STARTUP)
            # get sps pods in transition phase 1
            retList = restAPIutil.loadSpsPods()
            if len(retList.items) == 0:
                common.log(constants.LOG_LEVEL_DEBUG, "__restartSpsDeploymentAndWaitForNewReadyPods: no sps pods found (transition phase 1) ... try again")
            for item in retList.items:
                spsPodsNewNames.append(item.metadata.name)
            if len(retList.items) > 0:
                common.log(constants.LOG_LEVEL_DEBUG, "__restartSpsDeploymentAndWaitForNewReadyPods; sps pods names (transition phase 1): %s" % (spsPodsNewNames))
                # look for common elements
            if len(retList.items) > 0:
                spsCommonNames = set(spsPodsOldNames) & set(spsPodsNewNames)
                if len(spsCommonNames) == 0:
                    common.log(constants.LOG_LEVEL_DEBUG, "__restartSpsDeploymentAndWaitForNewReadyPods: there are only new pods (transition phase 1) ... go on")
                    break
                else:
                    common.log(constants.LOG_LEVEL_DEBUG, "__restartSpsDeploymentAndWaitForNewReadyPods: there are still old pods (transition phase 1) ... try again")
                    spsPodsNewNames.clear()

    # wait for the sps new "ready" pods
    # now there are all new pods
    # 2nd time step : to have at least one pod in ready state
    # this 2nd time step could be redundant
    if result == True:
        while True:
            # wait for 45 sec for sps pods start-up
            common.log(constants.LOG_LEVEL_DEBUG, "__restartSpsDeploymentAndWaitForNewReadyPods: (phase 2) sleep %s sec before ckecking if at least a new pod is ready" % (constants.SPS_WAIT_STARTUP))
            time.sleep(constants.SPS_WAIT_STARTUP)
            # get sps pods in transition phase 2
            retList = restAPIutil.loadSpsPods()
            if len(retList.items) == 0:
                common.log(constants.LOG_LEVEL_DEBUG, "__restartSpsDeploymentAndWaitForNewReadyPods: no sps pods found (transition phase 2) ... try again")
            foundPodReady = False
            if len(retList.items) > 0:
                # look for a POD with ready state
                # i.e. : all its containers (final containers and init-terminated containers) have to be ready
                for item in retList.items:
                    foundPodReady = True
                    for counter in range(len(item.status.container_statuses)):
                        # check state for container
                        if item.status.container_statuses[counter].ready == False:
                            foundPodReady = False
                            continue
                    if foundPodReady == True:
                        break
            if foundPodReady == True:
                common.log(constants.LOG_LEVEL_DEBUG, "__restartSpsDeploymentAndWaitForNewReadyPods: a new ready pod found (transition phase 2) ... go on")
                break
            else:
                common.log(constants.LOG_LEVEL_DEBUG, "__restartSpsDeploymentAndWaitForNewReadyPods: a new ready pod NOT found (transition phase 2) ... try again")

    if result == True:
        common.log(constants.LOG_LEVEL_DEBUG, "__restartSpsDeploymentAndWaitForNewReadyPods: ... END with SUCCESS")
    else:
        common.log(constants.LOG_LEVEL_WARNING, "__restartSpsDeploymentAndWaitForNewReadyPods: ... END with FAILURE")

    return result


###########################################
# __emptyTlsSecretsForCli
#
# descr: delete tlsStoreLocation, tlsStoreType and tlsStoreData fields
#        in the two tls scerets for cli java application
# NOTE: similar to __resetDataForCliTlsSecrets but here the reset of data is without other checks
#       present in the other function
#
# in: none
# out:  boolean
###########################################
def __emptyTlsSecretsForCli():

    common.log(constants.LOG_LEVEL_DEBUG, "__emptyTlsSecretsForCli: STARTING ...")

    # read tls secrets of cli
    common.log(constants.LOG_LEVEL_DEBUG, "__emptyTlsSecretsForCli: loading tls secrets for cli")
    result, retSecretsList = restAPIsecretUtil.loadTlsSecretsForCli()
    if result == False:
        common.log(constants.LOG_LEVEL_WARNING, "__emptyTlsSecretsForCli: loadTlsSecretsForCli failed ... exit ")
        return False

    cliStore1 = retSecretsList.items[0]
    cliStore2 = retSecretsList.items[1]
    secretName1 = cliStore1.metadata.name
    secretName2 = cliStore2.metadata.name

    # reset fields for the two secrets
    common.log(constants.LOG_LEVEL_DEBUG, "__emptyTlsSecretsForCli: delete fields for cli tls secrets")
    cliStore1 = restAPIsecretUtil.resetFieldsInTlsSecret(cliStore1)
    cliStore2 = restAPIsecretUtil.resetFieldsInTlsSecret(cliStore2)

    # update tls secrets of cli
    common.log(constants.LOG_LEVEL_DEBUG, "__emptyTlsSecretsForCli: updating tls secrets for cli")
    result = k8sApiInterface.updateSecretData(secretName1, constants.NAMESPACE, cliStore1)
    result = result and k8sApiInterface.updateSecretData(secretName2, constants.NAMESPACE, cliStore2)
    if result == False:
        common.log(constants.LOG_LEVEL_WARNING, "__emptyTlsSecretsForCli: failed to update tls secrets for cli")

    if result == True:
        common.log(constants.LOG_LEVEL_DEBUG, "__emptyTlsSecretsForCli: ... END with SUCCESS")
    else:
        common.log(constants.LOG_LEVEL_WARNING, "__emptyTlsSecretsForCli: ... END with FAILURE")

    return result


###########################################
# __internalCronJob
#
# descr: without waiting for expiration of the half hour,
#        while the normal cron job is still disabled,
#        for all the services that need for certs,
#        all cronjob operations are performed
#
# in: none
# out:  boolean
###########################################
def __internalCronJob():

    common.log(constants.LOG_LEVEL_DEBUG, "__internalCronJob: STARTING ...")
    result = True

    # Get the list of services that need for certs from credm controller.
    # Can not use restAPI.servicesList()
    retSecretsList = restAPIsecretUtil.loadAllCertReqSecrets()
    serviceNameList = []
    for item in retSecretsList.items:
        serviceName = item.metadata.labels[constants.SECRET_LABEL_SERVICENAME]
        if serviceName not in serviceNameList:
            serviceNameList.append(serviceName)
    common.log(constants.LOG_LEVEL_DEBUG, "__internalCronJob; list of services : %s" % serviceNameList)

    # for each service that needs for certs, all cronjob checks are performed.
    # Cronjob is executed for all services although there is a failure for any services
    for serviceName in serviceNameList:
        resultService = restAPI.periodicCheck(serviceName)
        if constants.RESULT_FROM_API_NOT_OK  in resultService:
            result = False
            common.log(constants.LOG_LEVEL_WARNING, "__internalCronJob; periodicCheck failure for service %s" % serviceName)
        else:
            common.log(constants.LOG_LEVEL_DEBUG, "__internalCronJob; periodicCheck success for service %s" % serviceName)

    if result == True:
        common.log(constants.LOG_LEVEL_DEBUG, "__internalCronJob: ... END with SUCCESS")
    else:
        common.log(constants.LOG_LEVEL_WARNING, "__internalCronJob: ... END with FAILURE")

    return result

#
# MAIN
#
# logging CONFIG
common.logConfig()

# k8s config
k8sApiInterface.k8sConfig()

# DEBUG (force a failure)
#prova = random.randint(0,9)
#common.log(constants.LOG_LEVEL_INFO, f"MS8 MS9 JOB: ... prova {prova}")
#if prova > 3:
#    sys.exit(1)

# run
common.log(constants.LOG_LEVEL_INFO, "MS8 MS9 JOB: ... START")
result = credmControllerTransitionFromEnablingToEnabled()
common.log(constants.LOG_LEVEL_INFO, "MS8 MS9 JOB: ... STOP")

if result:
    sys.exit(0)
else:
    sys.exit(1)

